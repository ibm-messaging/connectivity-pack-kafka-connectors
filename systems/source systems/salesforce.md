# Salesforce

The Salesforce connector enables streaming of Salesforce platform events and Change Data Capture (CDC) events by using the Faye client or Bayeux protocol. It also supports polling for custom objects using API-based polling to stream data from Salesforce, along with discovery of custom objects and properties.

## Pre-requisites

- Ensure streaming API is enabled for your Salesforce edition and organization.
- Ensure you have the required permissions set up in Salesforce to use Change Data Capture objects.
- Ensure you have the required permissions set up in Salesforce to access the specified objects and events.
- Set the Session Security Level at login value to `None` instead of `High Assurance`.
- To connect to Salesforce sandboxes or subdomains and use Salesforce as a source system to trigger events, enable the Salesforce Organization object in your Salesforce environment.
- If using Change Data Capture (CDC) events, ensure that CDC is enabled for the specified objects in Salesforce.

## Connecting to Salesforce

The `connectivitypack.source` and `connectivitypack.source.url` configurations in the `KafkaConnector` custom resource provide the connector with the required information to connect to the data source.

| **Name**                        | **Value or Description**                                                                                                        |
| ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| `connectivitypack.source`  | `salesforce` |
| `connectivitypack.source.url` | Specifies the URL of the source system. For example, for Salesforce, the base URL of your instance is `https://<yourinstance>.salesforce.com`. |

## Supported authentication mechanisms

You can configure the following authentication mechanisms for Salesforce in the `KafkaConnector` custom resource depending on the authentication flow in Salesforce.

### 1. Basic OAuth

| **Authentication Type** | **Use Case**   | **Required Credentials**                                                                                                                                                                                                                                                                                                                                                          | **KafkaConnector configuration and description** |
|-------------------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|
| Basic OAuth             | Recommended for most applications. | - **Client Identity:** Obtain this by creating a *Connected App* in Salesforce and locating the *Consumer Key* under the application's settings.<br><br> - **Client Secret:** Available in the *Connected App* configuration alongside the *Consumer Key*.<br><br> - **Access Token and Refresh Token:** Generated by performing an OAuth flow with the configured Connected App. | **`connectivitypack.source.credentials.authType: BASIC_OAUTH`** - Specifies that the connector will use Basic OAuth for authentication.<br><br> **`connectivitypack.source.credentials.clientSecret: <clientSecret>`** - Specifies the client secret of the Salesforce connected app used for Basic OAuth authentication. <br><br>  **`connectivitypack.source.credentials.clientIdentity: <clientIdentity>`** - Specifies the client ID (or consumer key) of the Salesforce connected app used for Basic OAuth authentication. <br><br> **`connectivitypack.source.credentials.accessTokenBasicOauth: <accessToken>`** - Specifies the access token used for Basic OAuth authentication with Salesforce. <br><br> **`connectivitypack.source.credentials.refreshTokenBasicOauth: <refreshToken>`** - Specifies the refresh token used to renew the OAuth access token for Basic OAuth authentication.|

For more information, see the [Salesforce OAuth 2.0 Documentation](https://developer.salesforce.com/docs/atlas.en-us.api_rest.meta/api_rest/intro_understanding_web_server_oauth_flow.htm).

### 2. OAuth2 Password (Deprecated)

**Important:** Salesforce has deprecated the OAuth2 Password grant type. If you're using this method, plan to migrate to Basic OAuth to ensure future compatibility.

| **Authentication Type**   | **Use Case**   | **Required Credentials** | **KafkaConnector configuration and description**   |
|---------------------------|-----------------|-------------------------------|-------------------------------|
| OAuth2 Password           | Legacy applications where Basic OAuth is not applicable. | - **Username and Password:** Use the Salesforce account’s credentials.<br><br> - **Client Identity and Client Secret:** Same as Basic OAuth, obtained from the *Connected App* settings.  | **`connectivitypack.source.credentials.authType: OAUTH2_PASSWORD`** - Specifies that the connector will use OAuth 2.0 Password authentication. <br><br> **`connectivitypack.source.credentials.username: <username>`** - Specifies the Salesforce username required for OAuth2 Password authentication. <br><br> **`connectivitypack.source.credentials.password: <password>`** - Specifies the Salesforce password associated with the username for OAuth2 Password authentication.  <br><br> **`connectivitypack.source.credentials.clientSecret: <clientSecret>`** - Specifies the client secret of the Salesforce Connected App required for OAuth2 Password authentication.  <br><br> **`connectivitypack.source.credentials.clientIdentity: <clientIdentity>`** - Specifies the client ID (or consumer key) of the Salesforce Connected App required for OAuth2 Password authentication.|

## Supported objects and events

You can specify any of the following objects and associated events in the `connectivitypack.source.objects` and the `connectivitypack.source.<object>.events` sections of the `KafkaConnector` custom resource. You can either provide one object or a list of comma separated objects.

### Platform Events

[Salesforce platform events](https://www.ibm.com/links?url=https%3A%2F%2Fdeveloper.salesforce.com%2Fdocs%2Fatlas.en-us.platform_events.meta%2Fplatform_events%2Fplatform_events_intro.htm) deliver custom event notifications when something meaningful happens to objects that are defined in your Salesforce organization. Platform events are dynamic in nature and specific to the endpoint account connected, and as a result are not shown in the static list.

|**Objects**  |           **Events**            |                   **Description**                   |                                             	**KafkaConnector configuration**                                             
|:------------:|:-------------------------------:|:---------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------:|
|Platform Event objects | `CREATED` | Captures creation of objects. | `connectivitypack.source.objects: <object>`<br>`connectivitypack.source.<object>.events: <CREATED>` |

#### Replay ID
Salesforce provides queues for recording platform events and each event notification has a unique replay ID. Salesforce retains platform events for 72 hours, and a user can store a replay ID value to use when subscribing again to retrieve events during the retention window, as described in the [Salesforce documentation](https://developer.salesforce.com/docs/atlas.en-us.api_streaming.meta/api_streaming/using_streaming_api_durability.htm).

The Salesforce connector uses the replay ID to track Salesforce platform events it has received. If the connector is restarted for any reason, it resumes streaming from where it stopped by using the replay ID. If the replay ID is no longer valid (more than 72 hours old), the connector will not be able to resume. Instead, it will start a new subscription to receive events from the current time.

### Change Data Capture Events

Salesforce CDC events provide notifications of state changes to objects that you are interested in.

**Note:** CDC must be enabled by customers, and it is only available for objects in the dynamic list.

All custom objects and a subset of standard objects are supported for use with Change Data Capture in Salesforce. For the full list, see [Change Event Object Support](https://www.ibm.com/links?url=https%3A%2F%2Fdeveloper.salesforce.com%2Fdocs%2Fatlas.en-us.change_data_capture.meta%2Fchange_data_capture%2Fcdc_object_support.htm).

|**Objects**  |           **Events**            |                   **Description**                   |                                             	**KafkaConnector configuration**                                             
|:------------:|:-------------------------------:|:---------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------:|
|Change Data Capture objects | `CREATED`, `UPDATED`, `DELETED` | Captures creation, updation or deletion of objects. | `connectivitypack.source.objects: <object>`<br>`connectivitypack.source.<object>.events: <CREATED>, <UPDATED>, <DELETED>` |


### Polling objects, events, and subscription parameters

The Salesforce polling connector captures events when custom objects are created or updated. These events are detected by polling connector at regular intervals.

|**Objects**  |   **Events**   |              **Description**              |                                                            	**KafkaConnector configuration**                                                            
|:------------:|:-------------:|:-----------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------:|
|Custom objects| `CREATED_POLLER`, `UPDATED_POLLER`, `CREATEDORUPDATED_POLLER` | Captures creation or updation of objects. | `connectivitypack.source.objects: <object>`<br>`connectivitypack.source.<object>.events: <CREATED_POLLER>, <UPDATED_POLLER>, <CREATEDORUPDATED_POLLER>` |


#### Subscription parameters

Subscription parameters configure how the Salesforce Polling connector polls for events. These parameters are specified for each object-event combination.

| **Parameter**| **Description**| **KafkaConnector configuration** |
|--------------|----------------|------------------------------------|
| `timeFormat`        | The date-time format to use when polling. For example, `YYYY-MM-DDTHH:mm:ssZ`. | `connectivitypack.source.<object>.<event>.subscription.timeFormat`   |
| `timeZone`          | The time zone for polling operations. Default value is UTC. For the complete list of supported time zone values, see [supported time zone values](../connectors/supported-timezones.md).| `connectivitypack.source.<object>.<event>.subscription.timezone`|
| `createdField`      | The field that contains the object’s creation timestamp. | `connectivitypack.source.<object>.<event>.subscription.createdField` |
| `updatedField`      | The field that contains the object’s last modified timestamp. | `connectivitypack.source.<object>.<event>.subscription.updatedField`  |
| `pollingInterval`   | The interval (in seconds) between polling requests. Default value is 5. Supported values are 1, 5, 10, 15, 30, 60.| `connectivitypack.source.<object>.<event>.subscription.pollingInterval`|

### Topic

Optional: The following parameter is used to configure the Kafka topic name.

|           **KafkaConnector configuration**            |                                                                                                                **Description**                                                                                                                |
|:----------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
|        `connectivitypack.topic.name.format`         | Specifies the format of the Kafka topic name generated for the event. The default value is `${object}-${eventType}`. You can give any format consisting of `${object}` and `${eventType}`. For example, `salesforce-<object>-CREATED_POLLER`. |

#### Heartbeat topic

Optional: The heartbeat topic verifies that the connector is active and operational. It contains only tombstone records — entries with no payload, consisting only timestamp. If heartbeat topic is specified, then Single Message Transformations (SMTs) to filter out tombstone records are not required. The following parameter is used to configure the name of the heartbeat topic.

|           **KafkaConnector configuration**            |                                                                                                                                                                                                                                                                                                                                                                     **Description**                                                                                                                                                                                                                                                                                                                                                                      |
|:----------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
|        `connectivitypack.source.heartbeat.topic`         | You can use the placeholder `${topic}` to dynamically insert the name of the actual topic to which the connector sends events. This placeholder is optional. If you specify a heartbeat topic name using `${topic}`, it will be replaced with the actual topic name. For example, if the topic is `salesforce-<object>-CREATED_POLLER` and the heartbeat topic is configured as `heartbeat-${topic}`, the resulting heartbeat topic will be `heartbeat-salesforce-<object>-CREATED_POLLER`. If you do not include `${topic}`, the specified heartbeat topic name will be used as-is. If no heartbeat topic is explicitly configured, the default heartbeat topic will be the same as the event topic, for example, `salesforce-<object>-CREATED_POLLER`. |

## Example configuration

The following is an example of a connector configuration for Salesforce:

```yaml
apiVersion: eventstreams.ibm.com/v1beta2
kind: KafkaConnector
metadata:
  labels:
    # The eventstreams.ibm.com/cluster label identifies the Kafka Connect instance
    # in which to create this connector. That KafkaConnect instance
    # must have the eventstreams.ibm.com/use-connector-resources annotation
    # set to true.
    eventstreams.ibm.com/cluster: cp-connect-cluster
  name: <name>
  namespace: <namespace>
spec:
  # Connector class name
  class: com.ibm.eventstreams.connect.connectivitypack.source.ConnectivityPackSourceConnector
  config:
    # Which data source to connect to, for example,  salesforce
    connectivitypack.source: salesforce
    # URL to access the data source,  for example, `https://<your-instance-name>.salesforce.com`
    connectivitypack.source.url: <URL-of-the-data-source-instance>
    # Credentials to access the data source using OAUTH2_PASSWORD authentication.
    connectivitypack.source.credentials.authType: OAUTH2_PASSWORD
    connectivitypack.source.credentials.username: <username>
    connectivitypack.source.credentials.password: <password>
    connectivitypack.source.credentials.clientSecret: <client-secret>
    connectivitypack.source.credentials.clientIdentity: <client-identity>
    # Objects and event types to read from the data source. For example, <object1> is a platform event object, <object2> is a CDC object and <object3> is a custom object. 
    connectivitypack.source.objects: '<object1>,<object2><object3>'
    connectivitypack.source.<object1>.events: 'CREATED'
    connectivitypack.source.<object2>.events: 'CREATED,UPDATED'
    connectivitypack.source.<object3>.events: 'CREATEDORUPDATED_POLLER'
    # Subscription parameters for polling. Required only for POLLER events
    connectivitypack.source.<object3>.CREATEDORUPDATED_POLLER.subscription.pollingInterval: 1
    connectivitypack.source.<object3>.CREATEDORUPDATED_POLLER.subscription.timezone: UTC
    connectivitypack.source.<object3>.CREATEDORUPDATED_POLLER.subscription.updatedField: LastModifiedDate
    connectivitypack.source.<object3>.CREATEDORUPDATED_POLLER.subscription.timeFormat: 'YYYY-MM-DDTHH:mm:ssZ'
    connectivitypack.source.<object3>.CREATEDORUPDATED_POLLER.subscription.createdField: CreatedDate
    # Optional, sets the format for Kafka topic names created by the connector.
    # You can use placeholders such as '${object}' and '${eventType}', which the connector will replace automatically.
    # Including '${object}' or '${eventType}' in the format is optional. For example, '${object}-topic-name' is a valid format.
    # By default, the format is '${object}-${eventType}', but it's shown here for clarity.
    connectivitypack.topic.name.format: '${object}-${eventType}'
    # Specifies the converter class used to deserialize the message value.
    # Change this to a different converter (for example, AvroConverter) as applicable.
    value.converter: org.apache.kafka.connect.json.JsonConverter
    # Controls whether the schema is included in the message.
    # Set this to false to disable schema support, or to true to enable schema inclusion (for example, for Avro).
    value.converter.schemas.enable: false
    # Optional, set the topic for producing the heartbeat events.
    connectivitypack.source.heartbeat.topic: heartbeat-${topic}
  # `tasksMax` must be equal to the number of object-eventType combinations
  # In this example it is 4 (object1 - CREATED, object2 - CREATED, object2 - UPDATED, object3 - CREATEDORUPDATED_POLLER)
  tasksMax: 4
```